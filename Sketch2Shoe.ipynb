{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb3d5a7",
   "metadata": {},
   "source": [
    "# Sketch2Shoe - Generating Photorealistic Footwear from Edge Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15d4527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NYUAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tarfile\n",
    "import urllib.request\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a5da3e",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3c4cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/edges2shoes.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"edges2shoes\"\n",
    "_URL = f\"http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz\"\n",
    "\n",
    "# Download and extract the dataset\n",
    "def download_and_extract(url, download_path):\n",
    "    if not download_path.exists():\n",
    "        print(f\"Downloading {url}...\")\n",
    "        tar_path, _ = urllib.request.urlretrieve(url)\n",
    "        print(f\"Extracting to {download_path}...\")\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=download_path.parent)\n",
    "    else:\n",
    "        print(f\"{dataset_name} already downloaded.\")\n",
    "\n",
    "# Set paths\n",
    "base_path = pathlib.Path('./data')\n",
    "download_path = base_path / dataset_name\n",
    "base_path.mkdir(exist_ok=True)\n",
    "\n",
    "download_and_extract(_URL, download_path)\n",
    "\n",
    "# Now dataset is available at:\n",
    "print(\"Dataset path:\", download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf5d35",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform  = T.Compose([\n",
    "    T.Resize(286, interpolation=InterpolationMode.BICUBIC),     # 256→286\n",
    "    T.RandomCrop((256, 256)),                       # crop height = 256, width = 256\n",
    "    T.RandomHorizontalFlip(p=0.5),                  # mirror left ↔ right\n",
    "    T.ToTensor(),                                   # [0,255] → [0,1]\n",
    "    T.Normalize((0.5,)*3, (0.5,)*3)                 # →[−1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', transform=None):\n",
    "        self.root = pathlib.Path(root) / mode\n",
    "        self.files = sorted(self.root.glob(\"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = PILImage.open(self.files[idx]).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        w2 = w // 2\n",
    "\n",
    "        # left = real, right = street\n",
    "        sketch_img   = img.crop((0, 0, w2, h))\n",
    "        shoe_img = img.crop((w2, 0, w,  h))\n",
    "\n",
    "        if self.transform:\n",
    "            sketch_img = self.transform(sketch_img)\n",
    "            shoe_img   = self.transform(shoe_img)\n",
    "\n",
    "        return sketch_img, shoe_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loaders\n",
    "train_ds = Pix2PixDataset('./data/edges2shoes', mode='train', transform=data_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True,  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d332fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(4, 2, figsize=(6, 12))\n",
    "for i, (sketch, shoe) in enumerate(train_dl):\n",
    "    if i == 4: break\n",
    "    \n",
    "    sk = sketch * 0.5 + 0.5\n",
    "    sh = shoe   * 0.5 + 0.5\n",
    "\n",
    "    axes[i, 0].imshow(sk[0].permute(1, 2, 0))\n",
    "    axes[i, 0].set_title(\"Sketch (Input)\")\n",
    "\n",
    "    axes[i, 1].imshow(sh[0].permute(1, 2, 0))\n",
    "    axes[i, 1].set_title(\"Shoe (Target)\")\n",
    "\n",
    "    for ax in axes[i]: \n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83921edd",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec97713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super().__init__()\n",
    "        # Since we concatenate two images, the first conv sees in_channels*2\n",
    "        self.model = nn.Sequential(\n",
    "        # → (in_channels*2) x 256 x 256\n",
    "        nn.Conv2d(in_channels * 2, base_features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → base_features x 128 x 128\n",
    "\n",
    "        nn.Conv2d(base_features, base_features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*2) x 64 x 64\n",
    "\n",
    "        nn.Conv2d(base_features*2, base_features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x 32 x 32\n",
    "\n",
    "        nn.ZeroPad2d(1),  \n",
    "        # → (base_features*4) x 34 x 34\n",
    "\n",
    "        nn.Conv2d(base_features*4, base_features*8, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*8) x 32 x 32\n",
    "\n",
    "        nn.ZeroPad2d(1),  \n",
    "        # → (base_features*8) x 34 x 34\n",
    "\n",
    "        # final “patch” conv; produces a 31×31 score map\n",
    "        nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "        # → 1 x 31 x 31\n",
    "        )\n",
    "\n",
    "        def forward(self, sketch_img, shoe_img):\n",
    "            x = torch.cat([sketch_img, shoe_img], dim=1)\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1852b8b",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        # --- ENCODER (downsampling) ---\n",
    "        # 256→128\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # 128→64\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 64→32\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 32→16\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 16→8\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 8→4\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 4→2\n",
    "        self.enc7 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 2→1  (bottleneck)\n",
    "        self.enc8 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- DECODER (upsampling) ---\n",
    "        # 1→2\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 2→4 (cat → 16 channels in)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 4→8\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 8→16\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 16→32 (cat 16+16→32 channels)\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 32→64\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*4*2, features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 64→128\n",
    "        self.dec7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2*2, features,   kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # final 128→256 & 3 channels\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)   # 256→128\n",
    "        e2 = self.enc2(e1)  # 128→64\n",
    "        e3 = self.enc3(e2)  # 64→32\n",
    "        e4 = self.enc4(e3)  # 32→16\n",
    "        e5 = self.enc5(e4)  # 16→8\n",
    "        e6 = self.enc6(e5)  # 8→4\n",
    "        e7 = self.enc7(e6)  # 4→2\n",
    "        e8 = self.enc8(e7)  # 2→1  (bottleneck)\n",
    "\n",
    "        # --- decode & concat skips ---\n",
    "        d1 = self.dec1(e8); d1 = torch.cat([d1, e7], dim=1)  # 1→2\n",
    "        d2 = self.dec2(d1); d2 = torch.cat([d2, e6], dim=1)  # 2→4\n",
    "        d3 = self.dec3(d2); d3 = torch.cat([d3, e5], dim=1)  # 4→8\n",
    "        d4 = self.dec4(d3); d4 = torch.cat([d4, e4], dim=1)  # 8→16\n",
    "        d5 = self.dec5(d4); d5 = torch.cat([d5, e3], dim=1)  # 16→32\n",
    "        d6 = self.dec6(d5); d6 = torch.cat([d6, e2], dim=1)  # 32→64\n",
    "        d7 = self.dec7(d6); d7 = torch.cat([d7, e1], dim=1)  # 64→128\n",
    "\n",
    "        return self.final(d7) # → 256×256×3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914b463",
   "metadata": {},
   "source": [
    "## Discriminator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, generator, sketch_images, shoe_images, opt_d):\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(show, real) should predict “real” → target=1\n",
    "    real_preds = discriminator(sketch_images, shoe_images)\n",
    "    real_targets = torch.ones_like(real_preds)\n",
    "    real_loss = F.binary_cross_entropy_with_logits(real_preds, real_targets)\n",
    "    real_score = real_preds.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(show) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_images = generator(sketch_images).detach()\n",
    "    fake_preds = discriminator(sketch_images, fake_images)\n",
    "    fake_targets = torch.zeros_like(fake_preds)\n",
    "    fake_loss    = F.binary_cross_entropy_with_logits(fake_preds, fake_targets)\n",
    "    fake_score   = fake_preds.mean().item()\n",
    "\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb0669",
   "metadata": {},
   "source": [
    "## Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator, generator, sketch_images, , opt_g, lambda_L1 = 100):\n",
    "    generator.train()\n",
    "    \n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    # 1) Adversarial loss\n",
    "    # Generate fake images\n",
    "    fake_images = generator(sketch_images)\n",
    "\n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(sketch_images, fake_images)\n",
    "    targets = torch.ones_like(preds)\n",
    "    adv_loss = F.binary_cross_entropy_with_logits(preds, targets)\n",
    "\n",
    "    # 2) L1 reconstruction loss\n",
    "    l1_loss = F.l1_loss(fake_images, shoe_images)\n",
    "\n",
    "    total_loss = adv_loss + (lambda_L1 * l1_loss)\n",
    "\n",
    "    # Update generator weights\n",
    "    total_loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return total_loss.item(), adv_loss.item(), l1_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d07787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717857dd",
   "metadata": {},
   "source": [
    "## Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Samples\n",
    "def save_samples(epoch_idx, sketch_batch, generator, denorm_fn, show=False):\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = generator(sketch_batch)\n",
    "\n",
    "    fake = denorm_fn(fake)\n",
    "    fname = f'generated-images-{epoch_idx:04d}.png'\n",
    "    save_image(fake, os.path.join('generated', fname), nrow=8)\n",
    "    print('Saved', fname)\n",
    "    \n",
    "    if show:\n",
    "        grid = make_grid(fake.cpu(), nrow=8)\n",
    "        plt.figure(figsize=(8,8)); plt.axis('off'); plt.imshow(grid.permute(1,2,0)); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6b739",
   "metadata": {},
   "source": [
    "## Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize from [-1,1] back to [0,1]\n",
    "def denorm(imgs):\n",
    "    return imgs * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    discriminator: nn.Module,\n",
    "    generator:     nn.Module,\n",
    "    train_dl,\n",
    "    fixed_sketches,           # a batch of sketch images, e.g. next(iter(val_dl))\n",
    "    denorm,                   # function mapping [-1,1]→[0,1]\n",
    "    device = None,\n",
    "    epochs = 200,\n",
    "    lr = 2e-4,\n",
    "    start_idx = 1\n",
    "):\n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "    for epoch in range(start_idx, start_idx + epochs):\n",
    "        sum_d = sum_g = 0.0\n",
    "        sum_real_s = sum_fake_s = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for sketches, shoes in tqdm(train_dl, desc=f\"Epoch {epoch}/{start_idx+epochs-1}\"):\n",
    "            # — Train D —\n",
    "            d_loss, real_s, fake_s = train_discriminator(\n",
    "                discriminator, generator,\n",
    "                sketches, shows,\n",
    "                opt_d\n",
    "            )\n",
    "\n",
    "            # — Train G —\n",
    "            g_loss, adv_loss, l1_loss = train_generator(\n",
    "                discriminator, generator,\n",
    "                sketches, shoes,\n",
    "                opt_g\n",
    "            )\n",
    "\n",
    "            sum_d       += d_loss\n",
    "            sum_g       += g_loss\n",
    "            sum_real_s  += real_s\n",
    "            sum_fake_s  += fake_s\n",
    "            n           += 1\n",
    "\n",
    "        # Averages\n",
    "        avg_d = sum_d / n\n",
    "        avg_g = sum_g / n\n",
    "        avg_real = sum_real_s / n\n",
    "        avg_fake = sum_fake_s / n\n",
    "\n",
    "        # Record losses & scores\n",
    "        losses_d.append(avg_d)\n",
    "        losses_g.append(avg_g)\n",
    "        real_scores.append(avg_real)\n",
    "        fake_scores.append(avg_fake)\n",
    "\n",
    "        # Log losses & scores (last batch)\n",
    "        print(\n",
    "            f\"Epoch [{epoch}]  \"\n",
    "            f\"loss_g: {avg_g:.4f}, loss_d: {avg_d:.4f}, \"\n",
    "            f\"real_score: {avg_real:.4f}, fake_score: {avg_fake:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save generated images\n",
    "        save_samples(epoch, fixed_sketches, generator, denorm, show=False)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(generator.state_dict(), f\"checkpoint_gen_epoch{epoch}.pth\")\n",
    "\n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff80783",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Pix2PixDiscriminator()\n",
    "generator     = Pix2PixGenerator()\n",
    "fixed_sketches, _ = next(iter(train_dl))\n",
    "lr = 0.0002\n",
    "epochs = 100\n",
    "\n",
    "history = fit(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    train_dl=train_dl,\n",
    "    fixed_sketches=fixed_sketches,\n",
    "    denorm=denorm,\n",
    "    device=None,  \n",
    "    epochs=100,\n",
    "    lr=2e-4,\n",
    "    start_idx=1\n",
    ")\n",
    "\n",
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77f51f",
   "metadata": {},
   "source": [
    "## Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator.state_dict(), 'G.pth')\n",
    "torch.save(discriminator.state_dict(), 'D.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cffd1",
   "metadata": {},
   "source": [
    "Here's how the generated images look, after the 1st, 5th, 10th, 50th, and 100th epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dfc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0005.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0010.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97900d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0050.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0100.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607efab",
   "metadata": {},
   "source": [
    "## Plotting Loss of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc35013",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = list(range(1, len(losses_d) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34473f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_d, label=\"Discriminator\")\n",
    "plt.plot(epochs_range, losses_g, label=\"Generator\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5074fb",
   "metadata": {},
   "source": [
    "## Plotting Real & Fake Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c79eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, real_scores, label=\"Real Score\")\n",
    "plt.plot(epochs_range, fake_scores, label=\"Fake Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Real vs Fake Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fac389",
   "metadata": {},
   "source": [
    "## Generating New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de521fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a validation loader\n",
    "val_ds  = Pix2PixDataset('./data/edges2shoes', mode='val', transform=data_transform)\n",
    "val_dl  = DataLoader(val_ds, batch_size=5, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59203bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Pix2PixGenerator()\n",
    "generator.load_state_dict(torch.load('G.pth'))\n",
    "generator.eval()\n",
    "\n",
    "# Sampling from val_dl\n",
    "sketches_batch, _ = next(iter(val_dl))\n",
    "fake_batch = generator(sketches_batch)\n",
    "fake_batch = denorm(fake_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images\n",
    "grid = make_grid(fake_batch, nrow=5).permute(1,2,0).squeeze()\n",
    "plt.figure(figsize=(25,3), dpi=100)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid, cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a1b1d",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to run your model end-to-end\n",
    "def translate(sketch_img: PILImage) -> PILImage:\n",
    "    x = data_transform(sketch_img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake = generator(x)\n",
    "\n",
    "    fake = (fake[0] * 0.5 + 0.5).clamp(0,1)\n",
    "    return ToPILImage()(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d386db",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=translate,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Input Sketch\"),\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Generated Shoe\"),\n",
    "    title=\"Sketch2Shoe\",\n",
    "    description=\"Upload a shoe sketch; get a photorealistic footwear image.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab40002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
