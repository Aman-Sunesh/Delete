{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de761177",
   "metadata": {},
   "source": [
    "# StreetViewGAN - Translating Between Maps and Real World Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce1f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NYUAD\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tarfile\n",
    "import urllib.request\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c7e34",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40858d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz...\n",
      "Extracting to data\\maps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NYUAD\\AppData\\Local\\Temp\\ipykernel_9392\\2686093772.py:11: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=download_path.parent)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: data\\maps\n"
     ]
    }
   ],
   "source": [
    "# dataset_name = \"maps\"\n",
    "# _URL = f\"http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz\"\n",
    "\n",
    "# # Download and extract the dataset\n",
    "# def download_and_extract(url, download_path):\n",
    "#     if not download_path.exists():\n",
    "#         print(f\"Downloading {url}...\")\n",
    "#         tar_path, _ = urllib.request.urlretrieve(url)\n",
    "#         print(f\"Extracting to {download_path}...\")\n",
    "#         with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "#             tar.extractall(path=download_path.parent)\n",
    "#     else:\n",
    "#         print(f\"{dataset_name} already downloaded.\")\n",
    "\n",
    "# # Set paths\n",
    "# base_path = pathlib.Path('./data')\n",
    "# download_path = base_path / dataset_name\n",
    "# base_path.mkdir(exist_ok=True)\n",
    "\n",
    "# download_and_extract(_URL, download_path)\n",
    "\n",
    "# # Now dataset is available at:\n",
    "# print(\"Dataset path:\", download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c26f2a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6c15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform  = T.Compose([\n",
    "    T.Resize(286, interpolation=InterpolationMode.BICUBIC),     # 256→286\n",
    "    T.RandomCrop((256, 256)),                       # crop height = 256, width = 256\n",
    "    T.RandomHorizontalFlip(p=0.5),                  # mirror left ↔ right\n",
    "    T.ToTensor(),                                   # [0,255] → [0,1]\n",
    "    T.Normalize((0.5,)*3, (0.5,)*3)                 # →[−1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3c95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', transform=None):\n",
    "        self.root = pathlib.Path(root) / mode\n",
    "        self.files = sorted(self.root.glob(\"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = PILImage.open(self.files[idx]).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        w2 = w // 2\n",
    "\n",
    "        # left = real, right = street\n",
    "        real_img   = img.crop((0, 0, w2, h))\n",
    "        street_img = img.crop((w2, 0, w,  h))\n",
    "\n",
    "        if self.transform:\n",
    "            street_img = self.transform(street_img)\n",
    "            real_img   = self.transform(real_img)\n",
    "\n",
    "        return real_img, street_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85f0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loaders\n",
    "train_ds = Pix2PixDataset('./data/maps', mode='train', transform=data_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True,  num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c14ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 18176, 4388) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      3\u001b[39m fig, axes = plt.subplots(\u001b[32m4\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m6\u001b[39m, \u001b[32m12\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# denormalize back to [0,1]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1297\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1296\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1298\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1299\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 18176, 4388) exited unexpectedly"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(6, 12))\n",
    "\n",
    "for i, (street, real) in enumerate(train_dl):\n",
    "    if i == 4: break\n",
    "\n",
    "    # denormalize back to [0,1]\n",
    "    st = street * 0.5 + 0.5\n",
    "    re = real   * 0.5 + 0.5\n",
    "\n",
    "    axes[i, 0].imshow(st[0].permute(1, 2, 0))\n",
    "    axes[i, 0].set_title(\"Street (Input)\")\n",
    "    axes[i, 1].imshow(re[0].permute(1, 2, 0))\n",
    "    axes[i, 1].set_title(\"Real (Target)\")\n",
    "\n",
    "    for ax in axes[i]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9242bb",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788615fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super().__init__()\n",
    "        # Since we concatwnate two images, the first conv sees in_channels*2\n",
    "        self.model = nn.Sequential(\n",
    "        # → (in_channels*2) x 256 x 256\n",
    "        nn.Conv2d(in_channels * 2, base_features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → base_features x 128 x 128\n",
    "\n",
    "        nn.Conv2d(base_features, base_features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*2) x 64 x 64\n",
    "\n",
    "        nn.Conv2d(base_features*2, base_features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*4) x 32 x 32\n",
    "\n",
    "        nn.ZeroPad2d(1),  \n",
    "        # → (base_features*4) x 34 x 34\n",
    "\n",
    "        nn.Conv2d(base_features*4, base_features*8, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "        nn.InstanceNorm2d(base_features*8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # → (base_features*8) x 32 x 32\n",
    "\n",
    "        nn.ZeroPad2d(1),  \n",
    "        # → (base_features*8) x 34 x 34\n",
    "\n",
    "        # final “patch” conv; produces a 31×31 score map\n",
    "        nn.Conv2d(base_features*8, 1, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "        # → 1 x 31 x 31\n",
    "        )\n",
    "\n",
    "    def forward(self, real_input, real_target):\n",
    "        x = torch.cat([real_input, real_target], dim = 1)  \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4748d",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f721f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        # --- ENCODER (downsampling) ---\n",
    "        # 256→128\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # 128→64\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 64→32\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 32→16\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 16→8\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 8→4\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 4→2\n",
    "        self.enc7 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 2→1  (bottleneck)\n",
    "        self.enc8 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- DECODER (upsampling) ---\n",
    "        # 1→2\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 2→4 (cat → 16 channels in)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 4→8\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 8→16\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 16→32 (cat 16+16→32 channels)\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*8*2, features*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 32→64\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*4*2, features*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 64→128\n",
    "        self.dec7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2*2, features,   kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # final 128→256 & 3 channels\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)   # 256→128\n",
    "        e2 = self.enc2(e1)  # 128→64\n",
    "        e3 = self.enc3(e2)  # 64→32\n",
    "        e4 = self.enc4(e3)  # 32→16\n",
    "        e5 = self.enc5(e4)  # 16→8\n",
    "        e6 = self.enc6(e5)  # 8→4\n",
    "        e7 = self.enc7(e6)  # 4→2\n",
    "        e8 = self.enc8(e7)  # 2→1  (bottleneck)\n",
    "\n",
    "        # --- decode & concat skips ---\n",
    "        d1 = self.dec1(e8); d1 = torch.cat([d1, e7], dim=1)  # 1→2\n",
    "        d2 = self.dec2(d1); d2 = torch.cat([d2, e6], dim=1)  # 2→4\n",
    "        d3 = self.dec3(d2); d3 = torch.cat([d3, e5], dim=1)  # 4→8\n",
    "        d4 = self.dec4(d3); d4 = torch.cat([d4, e4], dim=1)  # 8→16\n",
    "        d5 = self.dec5(d4); d5 = torch.cat([d5, e3], dim=1)  # 16→32\n",
    "        d6 = self.dec6(d5); d6 = torch.cat([d6, e2], dim=1)  # 32→64\n",
    "        d7 = self.dec7(d6); d7 = torch.cat([d7, e1], dim=1)  # 64→128\n",
    "\n",
    "        return self.final(d7) # → 256×256×3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ef237",
   "metadata": {},
   "source": [
    "## Discriminator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3039c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, generator, real_images, map_images, opt_d):\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # ——— Real pairs ———\n",
    "    # D(map, real) should predict “real” → target=1\n",
    "    real_preds = discriminator(real_images, map_images)\n",
    "    real_targets = torch.ones_like(real_preds)\n",
    "    real_loss = F.binary_cross_entropy_with_logits(real_preds, real_targets)\n",
    "    real_score = real_preds.mean().item()\n",
    "\n",
    "    # ——— Fake pairs ———\n",
    "    # Generate fake images\n",
    "    # G(map) → fake; detach so G’s grad isn’t updated here\n",
    "    fake_images = generator(real_images).detach()\n",
    "    fake_preds = discriminator(real_images, fake_images)\n",
    "    fake_targets = torch.zeros_like(fake_preds)\n",
    "    fake_loss    = F.binary_cross_entropy_with_logits(fake_preds, fake_targets)\n",
    "    fake_score   = fake_preds.mean().item()\n",
    "\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return loss.item(), real_score, fake_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa1429",
   "metadata": {},
   "source": [
    "## Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67caab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator, generator, map_images, real_images, opt_g, lambda_L1 = 100):\n",
    "    generator.train()\n",
    "    \n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    # 1) Adversarial loss\n",
    "    # Generate fake images\n",
    "    fake_images = generator(real_images)\n",
    "\n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(real_images, fake_images)\n",
    "    targets = torch.ones_like(preds)\n",
    "    adv_loss = F.binary_cross_entropy_with_logits(preds, targets)\n",
    "\n",
    "    # 2) L1 reconstruction loss\n",
    "    l1_loss = F.l1_loss(fake_images, real_images)\n",
    "\n",
    "    total_loss = adv_loss + (lambda_L1 * l1_loss)\n",
    "\n",
    "    # Update generator weights\n",
    "    total_loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return total_loss.item(), adv_loss.item(), l1_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57632f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f7676",
   "metadata": {},
   "source": [
    "## Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c84255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(index, map_batch, generator, denorm, show=True):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        fake = generator(map_batch)         \n",
    "\n",
    "    # Denormalize into [0,1] for saving/viewing\n",
    "    fake = denorm(fake)\n",
    "\n",
    "    fname = f'generated-images-{index:04d}.png'\n",
    "    save_image(fake, os.path.join(sample_dir, fname), nrow=8)\n",
    "    print('Saving', fname)\n",
    "\n",
    "    if show:\n",
    "        grid = make_grid(fake.cpu(), nrow=8)   # shape (3, H, W)\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(grid.permute(1,2,0)) # H×W×3\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38237e0",
   "metadata": {},
   "source": [
    "## Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize from [-1,1] back to [0,1]\n",
    "def denorm(imgs):\n",
    "    return imgs * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    discriminator: nn.Module,\n",
    "    generator:     nn.Module,\n",
    "    train_dl,\n",
    "    fixed_maps,           # a batch of map images, e.g. next(iter(val_dl))\n",
    "    denorm,               # function mapping [-1,1]→[0,1]\n",
    "    device = None,\n",
    "    epochs = 200,\n",
    "    lr = 2e-4,\n",
    "    start_idx = 1\n",
    "):\n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "    for epoch in range(start_idx, start_idx + epochs):\n",
    "        sum_d = sum_g = 0.0\n",
    "        sum_real_s = sum_fake_s = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for maps, reals in tqdm(train_dl, desc=f\"Epoch {epoch}/{start_idx+epochs-1}\"):\n",
    "            # — Train D —\n",
    "            d_loss, real_s, fake_s = train_discriminator(\n",
    "                discriminator, generator,\n",
    "                reals, maps,\n",
    "                opt_d\n",
    "            )\n",
    "\n",
    "            # — Train G —\n",
    "            g_loss, adv_loss, l1_loss = train_generator(\n",
    "                discriminator, generator,\n",
    "                maps, reals,\n",
    "                opt_g\n",
    "            )\n",
    "\n",
    "            sum_d       += d_loss\n",
    "            sum_g       += g_loss\n",
    "            sum_real_s  += real_s\n",
    "            sum_fake_s  += fake_s\n",
    "            n           += 1\n",
    "\n",
    "        # Averages\n",
    "        avg_d = sum_d / n\n",
    "        avg_g = sum_g / n\n",
    "        avg_real = sum_real_s / n\n",
    "        avg_fake = sum_fake_s / n\n",
    "\n",
    "        # Record losses & scores\n",
    "        losses_d.append(avg_d)\n",
    "        losses_g.append(avg_g)\n",
    "        real_scores.append(avg_real)\n",
    "        fake_scores.append(avg_fake)\n",
    "\n",
    "        # Log losses & scores (last batch)\n",
    "        print(\n",
    "            f\"Epoch [{epoch}]  \"\n",
    "            f\"loss_g: {avg_g:.4f}, loss_d: {avg_d:.4f}, \"\n",
    "            f\"real_score: {avg_real:.4f}, fake_score: {avg_fake:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save generated images\n",
    "        save_samples(epoch, fixed_maps, generator, denorm, show=False)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(generator.state_dict(), f\"checkpoint_gen_epoch{epoch}.pth\")\n",
    "\n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/1096 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 spatial element when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m lr = \u001b[32m0.0002\u001b[39m\n\u001b[32m      5\u001b[39m epochs = \u001b[32m200\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m history = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_maps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m losses_g, losses_d, real_scores, fake_scores = history\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(discriminator, generator, train_dl, fixed_maps, denorm, device, epochs, lr, start_idx)\u001b[39m\n\u001b[32m     26\u001b[39m n = \u001b[32m0\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m maps, reals \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx+epochs-\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# — Train D —\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     d_loss, real_s, fake_s = \u001b[43mtrain_discriminator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopt_d\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# — Train G —\u001b[39;00m\n\u001b[32m     37\u001b[39m     g_loss, adv_loss, l1_loss = train_generator(\n\u001b[32m     38\u001b[39m         discriminator, generator,\n\u001b[32m     39\u001b[39m         maps, reals,\n\u001b[32m     40\u001b[39m         opt_g\n\u001b[32m     41\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_discriminator\u001b[39m\u001b[34m(discriminator, generator, real_images, map_images, opt_d)\u001b[39m\n\u001b[32m     12\u001b[39m real_score = real_preds.mean().item()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ——— Fake pairs ———\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Generate fake images\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# G(map) → fake; detach so G’s grad isn’t updated here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m fake_images = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_images\u001b[49m\u001b[43m)\u001b[49m.detach()\n\u001b[32m     18\u001b[39m fake_preds = discriminator(map_images, fake_images)\n\u001b[32m     19\u001b[39m fake_targets = torch.zeros_like(fake_preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mPix2PixGenerator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    113\u001b[39m e6 = \u001b[38;5;28mself\u001b[39m.enc6(e5)  \u001b[38;5;66;03m# 8→4\u001b[39;00m\n\u001b[32m    114\u001b[39m e7 = \u001b[38;5;28mself\u001b[39m.enc7(e6)  \u001b[38;5;66;03m# 4→2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m e8 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc8\u001b[49m\u001b[43m(\u001b[49m\u001b[43me7\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 2→1  (bottleneck)\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# --- decode & concat skips ---\u001b[39;00m\n\u001b[32m    118\u001b[39m d1 = \u001b[38;5;28mself\u001b[39m.dec1(e8); d1 = torch.cat([d1, e7], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# 1→2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:124\u001b[39m, in \u001b[36m_InstanceNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[38;5;28mself\u001b[39m._get_no_batch_dim():\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_no_batch_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_instance_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:47\u001b[39m, in \u001b[36m_InstanceNorm._apply_instance_norm\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_instance_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/functional.py:2875\u001b[39m, in \u001b[36minstance_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[39m\n\u001b[32m   2862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2863\u001b[39m         instance_norm,\n\u001b[32m   2864\u001b[39m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2872\u001b[39m         eps=eps,\n\u001b[32m   2873\u001b[39m     )\n\u001b[32m   2874\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_input_stats:\n\u001b[32m-> \u001b[39m\u001b[32m2875\u001b[39m     \u001b[43m_verify_spatial_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.instance_norm(\n\u001b[32m   2877\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2878\u001b[39m     weight,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2885\u001b[39m     torch.backends.cudnn.enabled,\n\u001b[32m   2886\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/torch/nn/functional.py:2841\u001b[39m, in \u001b[36m_verify_spatial_size\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m   2839\u001b[39m     size_prods *= size[i]\n\u001b[32m   2840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_prods == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2842\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected more than 1 spatial element when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2843\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected more than 1 spatial element when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "discriminator = Pix2PixDiscriminator()\n",
    "generator     = Pix2PixGenerator()\n",
    "fixed_maps, _ = next(iter(train_dl))\n",
    "lr = 0.0002\n",
    "epochs = 50\n",
    "\n",
    "history = fit(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    train_dl=train_dl,\n",
    "    fixed_maps=fixed_maps,\n",
    "    denorm=denorm,\n",
    "    device=None,  \n",
    "    epochs=200,\n",
    "    lr=2e-4,\n",
    "    start_idx=1\n",
    ")\n",
    "\n",
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0c768",
   "metadata": {},
   "source": [
    "## Checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator.state_dict(), 'G.pth')\n",
    "torch.save(discriminator.state_dict(), 'D.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840b3a3",
   "metadata": {},
   "source": [
    "Here's how the generated images look, after the 1st, 5th, 10th, and 50th epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf06920",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df67876",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0005.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0010.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0050.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d55b5",
   "metadata": {},
   "source": [
    "## Plotting Loss of Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = list(range(1, len(losses_d) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79345e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, losses_d, label=\"Discriminator\")\n",
    "plt.plot(epochs_range, losses_g, label=\"Generator\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636032a",
   "metadata": {},
   "source": [
    "## Plotting Real & Fake Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs_range, real_scores, label=\"Real Score\")\n",
    "plt.plot(epochs_range, fake_scores, label=\"Fake Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Real vs Fake Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f07049",
   "metadata": {},
   "source": [
    "## Generating New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a validation loader\n",
    "val_ds  = Pix2PixDataset('./data/maps', mode='val', transform=data_transform)\n",
    "val_dl  = DataLoader(val_ds, batch_size=5, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Pix2PixGenerator()\n",
    "generator.load_state_dict(torch.load('G.pth'))\n",
    "generator.eval()\n",
    "\n",
    "# Sampling from val_dl\n",
    "maps_batch, _ = next(iter(val_dl))\n",
    "fake_batch = generator(maps_batch)\n",
    "fake_batch = denorm(fake_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images\n",
    "grid = make_grid(fake_batch, nrow=5).permute(1,2,0).squeeze()\n",
    "plt.figure(figsize=(25,3), dpi=100)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid, cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201ec01",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to run your model end-to-end\n",
    "def translate(real_img: PIL.Image.Image) -> PIL.Image.Image:\n",
    "    x = data_transform(real_img).unsqueeze(0)  # 1×3×256×256\n",
    "    with torch.no_grad():\n",
    "        fake = generator(x)   # 1×3×256×256\n",
    "    fake = denorm(fake[0]).clamp(0,1)   # back to [0,1]\n",
    "    return ToPILImage()(fake)  # PIL for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=translate,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Input Image\"),\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Generated Map\"),\n",
    "    title=\"StreetViewGAN\",\n",
    "    description=\"Upload a street view image; get the generated map.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
